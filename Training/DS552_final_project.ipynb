{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoSOEHgZWNyz"
   },
   "outputs": [],
   "source": [
    "# Zero shot captioning with CLIP\n",
    "#################################\n",
    "# Have BLIP generate multiple captions\n",
    "# Optionally, have GPT enhance these captions and make them more creative or longer\n",
    "# Feed these captions into CLIP to evaluate which text embedding is most similar to the image embedding\n",
    "# Give prompts such as \"funny\" or \"professional\" for different styles\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383,
     "referenced_widgets": [
      "e594aa018b744bb3be1058853a624575",
      "85f0876a9c674beebc9d7ec211624301",
      "1b864133fce947c5b067847d898ecf56",
      "9967b3abf33e4d22bdf8683780894414",
      "3f0ae5e6a34a4646acbe9b60a4a679f9",
      "b9a1b1fb1b844f2ba447a15f6630bf3b",
      "696a363a804b433997ce5053332001e0",
      "003bfe8a8777468db27449fb7caf9ac9",
      "093e9460cdd7496984973b98ab4b83d7",
      "f617cc102a1643edb61ab4fc0f94bde2",
      "24b4333e26bd4a11b545852e18d1f363",
      "11400dc7098e4868919f03bf01252736",
      "8e4d35cd44894f34a0a9ec2daf4b20fa",
      "8db14b47610946c0920f929fd9072d67",
      "bd090ef7f0de49e898937019f388e18c",
      "3bf3032253fe484ea8aaee49c854b0fd",
      "e19a08b05b1042d694320f361e036b6c",
      "4ea1a2d0fd234c21b18c11d9c3e44254",
      "172d27b81b6a4175b818826642884c5a",
      "8123c3143d6347d3a09561e2da98c6b2",
      "fb92409beccf45f597761fb1fbb8cdf6",
      "3cf67e67e98447c08601414d73cf0e64",
      "9375b9881226479d8be4b272759b9f48",
      "52d992788bf64c28bf229b412eb5f5f4",
      "396c38b591124ef6836ca26f371c005b",
      "7b35bd52beef48f5bb34f3a04c92adf5",
      "8df068aaf77140e3b99b358e717f67bc",
      "f2ac527e8b994965a0d68d1903cb6c23",
      "18878d7f41dc4c2ab1141923548483e0",
      "98317b07c87b402cb0eb84da893350c7",
      "35bf3bd5d4804421ae2e8f5561a7d515",
      "48fc817a21054f19879b42de9e43524d",
      "68ab3b2c205c4bb2bf54d90ebaca7959",
      "727139ad692f44e69d41188dce05590d",
      "c124ddd4cf3a496c991a831c6afae182",
      "7a20c865516345648ea3ec4b15b6ab17",
      "5f5ae4bd355c42ee91f821e8c3e6cc88",
      "3c4d005797724fce8e49a4ccb619ca30",
      "2c11ca43f4e74135bc0351d63a41ecc1",
      "7d2e95e8c0b147a69425499cc617b402",
      "a3df218226d24744a7b256fc156b8222",
      "bcdf51caa8134b87bdf7075a709804a4",
      "2082732dce894adea5ff8120f0d0ad9e",
      "ceba98d473a3428cb33bd91a85f7bff9",
      "87e7ab3c8271491ba1835b517915ca12",
      "3b92127831b64279974656d52336af63",
      "d3f353a00c224ad1ab15be40f336a7c6",
      "8455bac589bb4985a9a1cb17bb6644e2",
      "37be3720db80410193265290a7bc6d9f",
      "036dc5ef27054c13b838328051aec7f1",
      "e64c6f644edb4454b451e797d016ed4a",
      "204445ea85eb4a5aaaf67ba8b18e187b",
      "c72e84b59bf5409d93f0f41603b7ea57",
      "acfef822e07a48be8a23938413170590",
      "c48cc2f407b5448d9ec9b06c5b4d76b1",
      "26baae5157d548d7b55707c85e00a4db",
      "b7dacd3d83e94e8c91606024a3faeb5d",
      "febb6f656ce24bb9949b4e76f8f9e355",
      "2cf35ccd1fe54be08faf8b05e103e639",
      "1f870fe9d04a408989807490e6e1cd58",
      "bb5c8a33053244d09f513fe525879c5b",
      "f977590c773b40049040aed513266a8a",
      "0a206d9133d84115bb04b0c8e63cf3cc",
      "a57a8e9615be404fb2547079f10dc201",
      "008e4c28e93d4ef78a330f265b420b3e",
      "40b0e220515c471aa6a72c3aaa9b70c3",
      "e507a80b3f314cbf9d0646b2bba36bd9",
      "4da81df2d63f433b9695fa54c3f5f693",
      "1a8545f18059490291468b001f065da1",
      "0a84a1e8049c4c33b9c7cbbb868c7982",
      "51f405c166db44ddbd06c749e3dfc7cb",
      "5e93b9473e0a4cd5bf793da8236aff66",
      "d54f87b9680a43deb6c29284cb73f799",
      "14e2455cecd24595bc004e0efa2e96b0",
      "e79bcae59c904b2ba7d729c5c7e9cb55",
      "0d57e6b3f7a94f67adb17a649ae207fc",
      "db3f35d07c324ea386a12e9216686926"
     ]
    },
    "id": "TJKzcTVPcBG5",
    "outputId": "2369489a-34d7-4b98-e720-8ac6d41d6572"
   },
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXzQxSXIPaH2",
    "outputId": "0e4edf8a-0974-4891-e914-1dd5f5410d37"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "ds = load_dataset(\"Obscure-Entropy/ImageCaptioning_SmallParquets\",\n",
    "                  split=\"train\",\n",
    "                  streaming=True)\n",
    "\n",
    "# ds = load_dataset(\"Obscure-Entropy/ImageCaptioning_SmallParquets\", data_files=\"https://huggingface.co/datasets/Obscure-Entropy/ImageCaptioning_SmallParquets/blob/main/data/gbc_captions_0_100k.parquet\")\n",
    "# ds = load_dataset(\"Obscure-Entropy/ImageCaptioning_SmallParquets\", split=\"train[:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7c-rXl4QOZ2"
   },
   "outputs": [],
   "source": [
    "def transform(example):\n",
    "    # Convert image to RGB in case it's not in that format\n",
    "    image = example[\"img\"].convert(\"RGB\")\n",
    "    caption = example[\"en_cap\"]  # Get the caption text\n",
    "\n",
    "    # Use the processor to process both image and caption\n",
    "    inputs = processor(images=image, text=caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "    # Return pixel_values (processed image tensor) and input_ids (tokenized caption)\n",
    "    return {\n",
    "        \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),  # Image tensor\n",
    "        \"input_ids\": inputs[\"input_ids\"].squeeze(0),        # Tokenized caption\n",
    "        \"labels\": inputs[\"input_ids\"].squeeze(0)            # Tokenized caption\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rn-etu0aa90o",
    "outputId": "01d48ba7-73b9-4aad-a817-f36945e0204e"
   },
   "outputs": [],
   "source": [
    "small_ds = ds.take(10)\n",
    "\n",
    "processed_ds = [transform(sample) for sample in small_ds]\n",
    "\n",
    "print(processed_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkMbY3GTQb7o",
    "outputId": "2def5b02-f739-4e42-8e66-f847c8c47ed0"
   },
   "outputs": [],
   "source": [
    "# Apply preorocessing\n",
    "# dataset = ds.map(transform, remove_columns=[\"img\", \"en_cap\", \"hu_cap\"])\n",
    "\n",
    "# # Set the format for PyTorch\n",
    "# dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"])\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=processor.tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip-finetuned-captioning\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-6,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_ds,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "3-bQuMW3osTi",
    "outputId": "ccf755ce-7af9-4ef7-df44-53727836203b"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3n9uBzNmXD1c",
    "outputId": "3059e3d7-e02a-4b10-b594-8b3e9718dba3"
   },
   "outputs": [],
   "source": [
    "# Preprocess image and generate multiple captions\n",
    "captions = []\n",
    "image = Image.open(\"1922185828222748745.jpg\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 1. Greedy decoding\n",
    "out_greedy = model.generate(**inputs)\n",
    "captions.append((\"Greedy\", processor.decode(out_greedy[0], skip_special_tokens=True)))\n",
    "\n",
    "# 2. Beam search\n",
    "out_beam = model.generate(**inputs, num_beams=5, num_return_sequences=1)\n",
    "captions.append((\"Beam Search\", processor.decode(out_beam[0], skip_special_tokens=True)))\n",
    "\n",
    "# 3. Top-k sampling\n",
    "out_topk = model.generate(**inputs, do_sample=True, top_k=50, max_length=50)\n",
    "captions.append((\"Top-k Sampling\", processor.decode(out_topk[0], skip_special_tokens=True)))\n",
    "\n",
    "# 4. Top-p (nucleus) sampling\n",
    "out_topp = model.generate(**inputs, do_sample=True, top_p=0.9, max_length=50)\n",
    "captions.append((\"Top-p Sampling\", processor.decode(out_topp[0], skip_special_tokens=True)))\n",
    "\n",
    "# Display the results\n",
    "for method, caption in captions:\n",
    "    print(f\"[{method}] {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyFWH2z7cWnI"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPTNeoForCausalLM\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "enhancer_model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "enhancer_model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHulDRKzcwza",
    "outputId": "5e2b90cc-7896-4f5b-8465-2cde0205e66a"
   },
   "outputs": [],
   "source": [
    "# Enhance captions with GPT\n",
    "enhanced_captions = []\n",
    "for method, caption in captions:\n",
    "    # Set prompt\n",
    "    prompt = f\"Rewrite the following caption to make it more fun, engaging, and suitable for Instagram, keeping the original context intact:\\n\\\"{caption}\\\"\\nImproved:\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(enhancer_model.device)\n",
    "\n",
    "    # Get output\n",
    "    outputs = enhancer_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=input_ids.shape[1] + 50, # The input prompt + tokens for output\n",
    "        do_sample=True,\n",
    "        top_p=0.85,\n",
    "        temperature=0.5,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=2,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id, # Stop when the eos token is found\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the new caption\n",
    "    improved = generated.split(\"Improved:\")[-1].strip()\n",
    "    enhanced_captions.append(improved)\n",
    "\n",
    "# Print results\n",
    "for original, enhanced in zip(captions, enhanced_captions):\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Enhanced: {enhanced}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u9iVVpopoqo"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Initialize client with API key\n",
    "client = OpenAI(\n",
    "  api_key=\"<YOUR API KEY>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuWDNWEkrJ8k"
   },
   "outputs": [],
   "source": [
    "def generate_caption(prompt):\n",
    "    try:\n",
    "        # Make the API call to generate a caption\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-4o\",  # Specify the model you want to use\n",
    "            input=f\"Can you rewrite this caption to make it more fun and engaging for Instagram?\\n{prompt}\\nImproved:\",\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_output_tokens=50,  # Specify the maximum number of tokens for the generated response\n",
    "        )\n",
    "\n",
    "        # Extract the improved caption\n",
    "        improved_caption = response.output_text\n",
    "\n",
    "        return improved_caption\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating caption: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsjPD6sZvczR"
   },
   "outputs": [],
   "source": [
    "def generate_completion(prompt):\n",
    "  completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    store=True,\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": f\"Can you rewrite this caption to make it more fun and engaging for Instagram?\\n{prompt}\"}\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  return completion.choices[0].message;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KYVCY2DkszTO",
    "outputId": "30fa19a8-e792-4294-f8e3-1a934438870b"
   },
   "outputs": [],
   "source": [
    "# Take the greedy caption\n",
    "caption = \"there are two children laying on a bed holding a small piece of jewelry\"\n",
    "\n",
    "enhanced_caption = generate_completion(caption)\n",
    "\n",
    "print(enhanced_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLQWRGOBxECY",
    "outputId": "2a2dea81-426d-4321-e83b-6a905921ad5a"
   },
   "outputs": [],
   "source": [
    "print(enhanced_caption.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bN6_BhevotV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "\n",
    "def load_clip():\n",
    "    \"\"\"\n",
    "    Loads a pretrained CLIP model and processor\n",
    "\n",
    "    Returns: model, processor\n",
    "    \"\"\"\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        clip_model.to(\"cuda\")\n",
    "\n",
    "    return clip_model, clip_processor\n",
    "\n",
    "def select_best_caption_with_clip(clip_model, clip_processor, image_path, candidate_captions):\n",
    "    \"\"\"\n",
    "    Selects the caption most similar to the image using CLIP.\n",
    "\n",
    "    Args:\n",
    "        clip_model: pretrained CLIP model\n",
    "        clip_processor: CLIP processor to use\n",
    "        image_path (str): Path to the input image.\n",
    "        candidate_captions (List[str]): List of caption strings.\n",
    "\n",
    "    Returns:\n",
    "        (best_caption, all_scores): Tuple of the best caption and all similarity scores.\n",
    "    \"\"\"\n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Tokenize inputs\n",
    "    inputs = clip_processor(text=candidate_captions, images=image, return_tensors=\"pt\", padding=True).to(clip_model.device)\n",
    "\n",
    "    # Get image/text embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        image_embeds = outputs.image_embeds  # shape: (1, 512)\n",
    "        text_embeds = outputs.text_embeds    # shape: (num_captions, 512)\n",
    "\n",
    "    # Normalize\n",
    "    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Cosine similarity\n",
    "    similarity_scores = (image_embeds @ text_embeds.T).squeeze(0)  # shape: (num_captions,)\n",
    "\n",
    "    # Select best\n",
    "    best_idx = similarity_scores.argmax().item()\n",
    "    best_caption = candidate_captions[best_idx]\n",
    "\n",
    "    return best_caption, similarity_scores.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NaenuiYjvuru",
    "outputId": "9a9fc1a2-efc6-4dba-fc26-dab2c03ca00f"
   },
   "outputs": [],
   "source": [
    "clip_model, clip_processor = load_clip()\n",
    "\n",
    "best_caption, scores = select_best_caption_with_clip(clip_model, clip_processor, \"1922185828222748745.jpg\", captions)\n",
    "\n",
    "print(\"Best Caption:\", best_caption)\n",
    "print(\"Scores:\", scores)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
